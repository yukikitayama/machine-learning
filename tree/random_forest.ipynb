{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d191151",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "How does random forest generate the forest? Why would we use random forest over other algorithms such as logistic regression?\n",
    "\n",
    "Random forest mitigates overfitting. It creates many randomly generated decision trees and take the majority vote. Each decision tree is made by subset of rows in data and subset of features in data.\n",
    "\n",
    "Decision trees have nodes to split data. The quality of the split is measured by entropy or Gini impurity metrics to find the likelihood that a data point will be misclassified. Decision tree works better with discrete values. With continuous data, it increases the number of possible models of splitting data to test whether split was good. Decision tree is likely to overfit to the data, but random forest generates multiple trees to mitigate overfitting.\n",
    "\n",
    "Random forecast performs better with discrete or easily discretizible data, but the logistic regression could perform better with the continuous value data. \n",
    "\n",
    "Random forest has trees trained on a portion of the entire dataset so that it's trained with the edge cases to have unusual and deeper patterns in data. But other models tend to take the entire data all at once for training, so reducing the ability to detect edge cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
