{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd355f55",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddebf0",
   "metadata": {},
   "source": [
    "## d-prime\n",
    "\n",
    "A measure of discrimination. It distinguishes between all yes responses (hits + false alarms) and *correct* yes responses (hits only).\n",
    "\n",
    "$p(H)$ is probability of hits. It's a conditional probability of having yes given it's positive.\n",
    "\n",
    "$$\n",
    "p(H) = \\frac{\\text{Hit}}{\\text{Hit} + \\text{Miss}}\n",
    "$$\n",
    "\n",
    "$p(\\text{FA})$ is probability of false alarm. CR is correct rejection.\n",
    "\n",
    "$$\n",
    "p(\\text{FA}) = \\frac{\\text{FA}}{\\text{FA} + \\text{CR}}\n",
    "$$\n",
    "\n",
    "Convert thoes probabilities to z-score; $z(H)$, $z(FA)$, by **normal cumulative density function** $scipy.stats.norm.ppf()$\n",
    "\n",
    "$$\n",
    "d' = z(H) - z(FA)\n",
    "$$\n",
    "\n",
    "d-prime is good if between 1 and 2.\n",
    "\n",
    "- 0 is pure guessing\n",
    "- 1 is good\n",
    "- 2 is awesome\n",
    "\n",
    "## Response bias\n",
    "\n",
    "The tendency to respond \"Yes\" more often or \"No\" more often. \n",
    "\n",
    "Response bias is orthogonal to (unrelated to) d-prime, because very different d-primes can be associated with the same bias.\n",
    "\n",
    "Computing response bias is by taking the negative average,\n",
    "\n",
    "$$\n",
    "\\text{Response bias} = - \\frac{z(FA) + z(H)}{2}\n",
    "$$\n",
    "\n",
    "- When response bias is around 0, **no bias**. In such case, $p(FA)$ and $p(H)$ are around 0.5, so $z(FA)$ and $z(H)$ will be 0.\n",
    "- When response bias is close to 1, **prefer to say \"No\"**.\n",
    "- When response bias is close to -1, **prefer to say \"Yes\"**.\n",
    "\n",
    " \n",
    "\n",
    "## F-score\n",
    "\n",
    "$$\n",
    "\\text{F}_1 \\text{score} = \\frac{2 \\times \\text{Precision} + \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{2 \\times \\frac{\\text{Hits}}{\\text{Hits} + \\text{FA}} \\times \\frac{\\text{Hits}}{\\text{Hits} + \\text{Miss}}}{\\frac{\\text{Hits}}{\\text{Hits} + \\text{FA}} + \\frac{\\text{Hits}}{\\text{Hits} + \\text{Miss}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\n",
    "\\frac{ 2 \\times \\text{Hits} \\times \\text{Hits}\n",
    "}\n",
    "{\n",
    "(\\text{Hits} + \\text{FA}) (\\text{Hits} + \\text{Miss})\n",
    "}\n",
    "}\n",
    "{\\frac{\n",
    "\\text{Hits} (\\text{Hits} + \\text{Miss}) + \\text{Hits} (\\text{Hits} + \\text{FA})\n",
    "} \n",
    "{\n",
    "(\\text{Hits} + \\text{FA}) (\\text{Hits} + \\text{Miss})\n",
    "}\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\n",
    "2 \\times \\text{Hits} \\times \\text{Hits}\n",
    "}\n",
    "{\n",
    "\\text{Hits} (\\text{Hits} + \\text{Miss}) + \\text{Hits} (\\text{Hits} + \\text{FA})\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\n",
    "2 \\times \\text{Hits}\n",
    "}\n",
    "{\n",
    "(\\text{Hits} + \\text{Miss}) + (\\text{Hits} + \\text{FA})\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\n",
    "\\text{Hits}\n",
    "}\n",
    "{\n",
    "\\text{Hits} + \\frac{(\\text{Miss} + \\text{FA})}{2}\n",
    "}\n",
    "$$\n",
    "\n",
    "If we make random guessing, we expect to have the same numbers of hits, false alarm, miss and correct rejection, and then $\\text{F}_1 \\text{score}$ will be 0.5. For example, if we have 10 hits, and miss and false alarms are also 10s, so 10 divided by 20 is 0.5. \n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{Hits}}{\\text{Hits} + \\text{FA}}\n",
    "$$\n",
    "\n",
    "This is a normalized measure of the number of hits, normalized by the False Alarm. It's about how much accurate we are about the predicted positive.\n",
    "\n",
    "### Recall\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{Hits}}{\\text{Hits} + \\text{Miss}}\n",
    "$$\n",
    "\n",
    "This is also a normalized measure of the number of hits, but by the number of missing positive prediction. It's about how much accurate we are about the actual positive.\n",
    "\n",
    "## ROC curve\n",
    "\n",
    "ROC curve is about the positive space. Y-axis is **probability of hits** (correct positive). X-axis is **probability of false alarm** (incorrect positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e9cc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d-prime: 1.90\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "hit = 22 / 30\n",
    "fa = 3 / 30\n",
    "\n",
    "hit_z = stats.norm.ppf(hit)\n",
    "fa_z = stats.norm.ppf(fa)\n",
    "\n",
    "d_prime = hit_z - fa_z\n",
    "\n",
    "print(f'd-prime: {d_prime:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00e672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1eef7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1fba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0444aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
