{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2271080d",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "## Background\n",
    "\n",
    "Data scientists sometimes need to do experiments about user interface in web design and product marketing. A/B testing is a common approach to test the hypothesis. For example, whether a new interface B is better than the existing interface A.\n",
    "\n",
    "**Control group** is a standard, existing, without-new-things group.\n",
    "\n",
    "**Treatment group (Test group)** is a group with a new feature or a specific thing we want to test. \n",
    "\n",
    "A/B testing typically measures, for example\n",
    "\n",
    "- Number of clicks\n",
    "- Number of purchases\n",
    "- Duration of seeing web apges\n",
    "\n",
    "## Hypothesis test (Significance test)\n",
    "\n",
    "Something that A/B testing usually establishes. When we see difference between A and B, the hypothesis test will tell us *whether it's due to random chance or true difference*. \n",
    "\n",
    "A/B testing typically uses **one-way** hypothesis test (**one-tail** hypothesis test). One-way hypothesis test treats one group as default and test whether the other group is better.\n",
    "\n",
    "**Z-test** and **t-test** are hypothesis test. Both are the *same* in the following points.\n",
    "\n",
    "- Decide if the **mean** from the sample is different from a certain value (or more than a value, or less than a value)\n",
    "- Assume the samples follow a **normal distribution**.\n",
    "- Use the same test statistic below. $s$ is the standard deviation. $n$ is the sample size\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\mu} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n",
    "$$\n",
    "\n",
    "But they are *different* in the following.\n",
    "\n",
    "Z-test uses **standard normal distribution**.\n",
    "\n",
    "t-test uses **t-distribution**. It has **degrees of freedom**. As it gets large, t-distribution gets closer to standard normal distribution. When the sample size is small, t-test is more appropriate than Z-test.\n",
    "\n",
    "In hypothesis test, 2 types of error can occur\n",
    "\n",
    "### Type I error\n",
    "\n",
    "Incorrectly conclude that an effect is real when it's actually just due to chance.\n",
    "\n",
    "Reject the null hypothesis when the null hypothesis is true.\n",
    "\n",
    "For example, the hypothesis test is about whether a drug has an effect on a patient. Null hypothesis is that the drug has no effect. The alternative hypothesis is that the drug has an effect. Type I error in this hypothesis test is to conclude that the drug has an effect (when the drug has no effect).\n",
    "\n",
    "It's also called **false positive rate**.\n",
    "\n",
    "In the hypothesis test, we typically don't wanna be fooled by the random chance, so we try to minimize this Type I error more than the following Type II error.\n",
    "\n",
    "### Type II error\n",
    "\n",
    "Incorrectly conclude that an effect is not real, so the effect is just due to chance, but it's actually real.\n",
    "\n",
    "Fail to reject the null hypothesis when the null hypothesis is false.\n",
    "\n",
    "For example, in the same testing a drug, Type II error is to conclude that the drug has no effect, when the drug actually has effect.\n",
    "\n",
    "It's also called **false negative rate**.\n",
    "\n",
    "### p-value\n",
    "\n",
    "Given a null hypothesis model, the probability that results as extreme as the observed results might occur.\n",
    "\n",
    "Given a chance model, p-value is the probability that a result is this extreme.\n",
    "\n",
    "In random permutation test, the proportion of times that the test produces a difference equal to or greater than the observed difference.\n",
    "\n",
    "The wrong interpretation of p-value is the probability that the result is due to chance.\n",
    "\n",
    "## Random permutation test\n",
    "\n",
    "Also called, **randomization test**, **resampling permutation procedure**, **bootstrap permutation test**.\n",
    "\n",
    "**Permute** means to change the order of a set of values.\n",
    "\n",
    "Different way from Z-test and t-test to test whether an observed test statistic is significant or not. I describe the methodology here, but I think looking at the below code is easier to understand.\n",
    "\n",
    "1. Combine group A and B, shuffle and draw sample with the same size as the number of data in group A.\n",
    "2. The remaining data is group B.\n",
    "3. Compute a test statistic and save it\n",
    "4. Repeat above steps many times to create a distribution\n",
    "5. The distribution works as the reference null hypothesis distribution.\n",
    "6. Compare the original observed test statistic with this distribution to compare p-value.\n",
    "\n",
    "*Useful when samples don't follow a normal distribution*. So when we can't use Z-test or t-test, we can use random permutation test instead.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame('web_page_data.csv')\n",
    "```\n",
    "\n",
    "![Web page data](./web_page_data.png)\n",
    "\n",
    "```python\n",
    "mean_a = df.loc[df['Page'] == 'Page A', 'Time'].mean()\n",
    "mean_b = df.loc[df['Page'] == 'Page B', 'Time'].mean()\n",
    "observed_difference = mean_b - mean_a\n",
    "print(f'Observed difference: {observed_difference}')\n",
    "\n",
    "\n",
    "def permute_and_compute_difference(df, n_a, n_b):\n",
    "    # Assumes dataframe has index from 0 to n - 1\n",
    "    indices = [i for i in range(n_a + n_b)]\n",
    "    index_b = set(random.sample(indices, n_b))\n",
    "    index_a = set(indices) - index_b\n",
    "    mean_a = df.loc[index_a, 'Time'].mean()\n",
    "    mean_b = df.loc[index_b, 'Time'].mean()\n",
    "    return mean_b - mean_a\n",
    "\n",
    "\n",
    "n_a = len(df.loc[df['Page'] == 'Page A'])\n",
    "n_b = len(df.loc[df['Page'] == 'Page B'])\n",
    "num_sampling = 1000\n",
    "permuted_differences = []\n",
    "\n",
    "for _ in range(num_sampling):\n",
    "    difference = permute_and_compute_difference(df, n_a, n_b)\n",
    "    permuted_differences.append(difference)\n",
    "\n",
    "p_value = np.mean(observed_difference < permuted_differences)\n",
    "```\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.hist(\n",
    "    permuted_differences, \n",
    "    bins=30,\n",
    "    label='Permuted differences'\n",
    ")\n",
    "plt.axvline(\n",
    "    observed_difference,\n",
    "    color='tab:orange',\n",
    "    linestyle='--',\n",
    "    label='Observed difference'\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Random permutation test](./random_permutation_test.png)\n",
    "\n",
    "## Multiple A/B tests\n",
    "\n",
    "We can run multiple A/B tests, but as we run multiple testing, it increases the probability of making Type I error. This is called **alpha inflation**.\n",
    "\n",
    "When $\\alpha$ is the probability that falsely test is significant, $(1 - \\alpha)$ is the probability that correctly test is not significant. When we run $n$ tests and all are correctly tested as non-significant, the probability is $(1 - \\alpha)^n$. So the probability that at least one test will falsely test significant is $1 - (1 - \\alpha)^n$. When $\\alpha = 0.05$ and $n = 20$, \n",
    "\n",
    "$$\n",
    "1 - (1 - 0.05)^{20}\n",
    "$$\n",
    "$$\n",
    "= 1 - 0.95^{20}\n",
    "$$\n",
    "$$\n",
    "= 1 - 0.36\n",
    "$$\n",
    "$$\n",
    "= 0.64\n",
    "$$\n",
    "\n",
    "So the probability of making at least one type 1 error in multiple testing is high.\n",
    "\n",
    "For multiple A/B testing, we can try **Bonferroni correction**, **F-test**, and **multi-arm bandit**. \n",
    "\n",
    "### Bonferroni correction (adjustment)\n",
    "\n",
    "Divide significance level $\\alpha$ by the number of multiple test $n$, so $\\frac{\\alpha}{n}$.\n",
    "\n",
    "In Bonferroni correction, when we run $n$ tests with significance level $\\alpha$, we use a significance level $\\frac{\\alpha}{n}$ instead. Even if all the hypotheses are false positive, the significance level is $\\alpha$.\n",
    "\n",
    "The disadvantage is to increase false negative. It's type II error. It's problematic if false positive is preferable to false negative. For example, in healthcare, false positive result of detecting disease gives concern and more tests might be required, but false negative means ignoring a potentially fetal disease.\n",
    "\n",
    "### F-test\n",
    "\n",
    "F-test can run hypothesis tests at once. But F-test only tests us whether there is a difference in mean between at least 2 groups, and it doesn't tell us how much differ, which groups, how many groups.\n",
    "\n",
    "The disadvantage is the same as above Bonferroni correction.\n",
    "\n",
    "### Multi-arm bandit algorithm\n",
    "\n",
    "As we proceed making tests, suppose we have variant A, B, and C, if we find A is better, we start using A in production more often, but by random manner, sometimes we are still use B and C. Continuously doing this try and error.\n",
    "\n",
    "**Epsilon-greedy algorithm for A/B test**\n",
    "\n",
    "1. Set epsilon somewhere between 0 and 1, but to be a small number.\n",
    "2. Get a random number between 0 and 1.\n",
    "3. If the random number < epsilon, flip a coin\n",
    "  1. If head, use A\n",
    "  2. If tail, use B\n",
    "4. If the random number $\\ge$ epsilon, use an option which gives us the better result so far.\n",
    "\n",
    "When epsilon is 1, it's a regular A/B testing, random allocation. When epsilon is 0, it's a greedy algorithm. Always choose the best variant so far, no more experiment.\n",
    "\n",
    "## Compute sample size\n",
    "\n",
    "We can compute sample size if we have **effect size**, **power**, and **significance level**.\n",
    "\n",
    "The **effect size** is the amount of change we hope to have with a new feature. For example, with a new feature, if we want the click rate to increase by 10%, the 10% is the effect size.\n",
    "\n",
    "The **power** is the probability of detecting the effect size in A/B testing.\n",
    "\n",
    "The **significance level** is the probability of incorrectly saying we have the effect when the effect actually doesn't exist.\n",
    "\n",
    "## Experimental design\n",
    "\n",
    "### Multiple features\n",
    "\n",
    "Button color and button location\n",
    "\n",
    "### Interpret A/B testing result\n",
    "\n",
    "new UI effect\n",
    "\n",
    "### Apply A/B testing to price\n",
    "\n",
    "### Randomization\n",
    "\n",
    "Random bucketing\n",
    "\n",
    "### Non-normal distribution\n",
    "\n",
    "Mann-whitney, etc.\n",
    "\n",
    "## Terminology\n",
    "\n",
    "- Churn rate\n",
    "  - The rate at which a customer stops doing business with a company, or stops subscribing to a service.\n",
    "- Researcher bias\n",
    "  - Selecting a test statistic after an experiment, but the test statistic needs to be established beforehand. \n",
    "\n",
    "## Reference\n",
    "\n",
    "- O'REILLY Practical Statistics for Data Scientist, 50+ Essential Concepts Using R and Python\n",
    "- [How to A/B Test Your Pricing (And Why It Might Be a Bad Idea)](https://blog.hubspot.com/marketing/price-testing)\n",
    "- [Common Mistakes During A/B Testing](https://towardsdatascience.com/common-mistakes-during-a-b-testing-bdb9eefdc7f0)\n",
    "- [Type I & Type II Errors | Differences, Examples, Visualizations](https://www.scribbr.com/statistics/type-i-and-type-ii-errors/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
