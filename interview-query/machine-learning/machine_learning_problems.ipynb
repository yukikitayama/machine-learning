{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551ec35a",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71d6ca",
   "metadata": {},
   "source": [
    "## Rejection reason\n",
    "\n",
    "### Question\n",
    "\n",
    "Suppose we have a binary classification model that classifies whether or not an applicant should be qualified to get a loan. Because we are a financial company, we have to provide each rejected applicant with a reason why. Given we don’t have access to the feature weights, how would we give each rejected applicant a reason why they got rejected?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Create **partial dependence plot** for each feature and find the value in a feature which increases the probability of default, and explain to applicants that their input to a certain feature possibly was the reason for rejection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfe928",
   "metadata": {},
   "source": [
    "## Keyword bidding\n",
    "\n",
    "### Question\n",
    "\n",
    "Let’s say you’re working on keyword bidding optimization. You’re given a dataset with two columns. One column contains the keywords that are being bid against, and the other column contains the price that’s being paid for those keywords. Given this dataset, how would you build a model to bid on a new unseen keyword?\n",
    "\n",
    "### Answer\n",
    "\n",
    "We need to build a supervised learning algorithm which takes keyword column as input and outputs the bidding price.\n",
    "\n",
    "We make word embeddings where similar words have a similar representation in the form of vectors.\n",
    "\n",
    "Take cosine similarity of words to a target word and recommend prices of the similar words.\n",
    "\n",
    "Todo: GloVe embedding, Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbca689",
   "metadata": {},
   "source": [
    "## 85% vs 82%\n",
    "\n",
    "### Question\n",
    "\n",
    "We have two models: one with 85% accuracy, one 82%. Which one do you pick?\n",
    "\n",
    "### Answer\n",
    "\n",
    "We need to know whether a higher accuracy or a higher interpretable model is important to the business, because 85% accuracy model could be from a blackbox model and 82% accuracy model could be linear regression.\n",
    "\n",
    "If this model is a binary classification model, and recall or precision are important to the business, we might pick a model regardless of the accuracy. 82% accuracy model could have a higher recall or precision than 85% accuracy model.\n",
    "\n",
    "We also need to know the scalability. 85% accuracy model could have too long training time and model could use too much memory, so that 85% accuracy model cannot be used in production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f32f4",
   "metadata": {},
   "source": [
    "## Multicollinearity in regression\n",
    "\n",
    "### Question\n",
    "\n",
    "How would you tackle multicollinearity in multiple linear regression?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Ignore multicollinearity, if the predictions work in training and test dateset, and the correlation of variables are not that high.\n",
    "\n",
    "If the multicollinearity is caused by higher-order terms, standardize the variables.\n",
    "\n",
    "Reduce the number of vairables. Remove one of the highly correlated variables. Apply PCA to reduce the dimension.\n",
    "\n",
    "### Resource\n",
    "\n",
    "- [When Do You Need to Standardize the Variables in a Regression Model?](https://statisticsbyjim.com/regression/standardize-variables-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618c5ea-1248-4cd7-ba7e-860a3a33e283",
   "metadata": {},
   "source": [
    "## Variate anomalies\n",
    "\n",
    "### Question\n",
    "\n",
    "If given a univariate dataset, how would you design a function to detect anomalies? What if the data is bivariate?\n",
    "\n",
    "### Answer\n",
    "\n",
    "In a function, it can find the values at the 1th and 99th percentiles for example, and eliminate all values below or above those thresholds. In a bivariate data, anomaly detection can be one variable individually, or a combination of 2 variables. The example of anomaly detection machine learning algorithm is **Isolation Forecast**, **DBSCAN**, **Bayesian Gaussian Mixture**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a0fba-c1b6-40d0-afc4-26422b38b4d5",
   "metadata": {},
   "source": [
    "## Explaining linear regression to different audiences\n",
    "\n",
    "### Question\n",
    "\n",
    "How would you explain the concept of linear regression to a child, a first-year college student and a mathematician?\n",
    "\n",
    "### Answer\n",
    "\n",
    "To child, draw a lot of points, and draw one straight line in the center of the group of points.\n",
    "\n",
    "To college student, linear regression is a statistical model to predict numbers. It's a way to draw a straight line in a scatter plot of data points, such that the straight line minimizes the distance between the actual data and the predicted data.\n",
    "\n",
    "To mathematician, linear regression is a method to model relationship between a dependent variable $y$ and one or more independent variables $X$. The method assumes the linear relationship between $y$ and $X$, so that $y$ will be calculated by a linear combination of $X$. This linear combination minimizes the distance between the actual values and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f11364-6182-4e84-975f-79fa55f1335a",
   "metadata": {},
   "source": [
    "## Pizza no show\n",
    "\n",
    "### Question\n",
    "\n",
    "You run a pizza shop, and you run into a lot of no-shows after customers place their order. What features would you include in a model to try to predict a no-show?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Some of the following variables may be difficult to obtain. It depends on discussion with business team and technical team about the feasibility of getting those data.\n",
    "\n",
    "For business level, order type (call, online, in-person), employee administering order, order cost, projected time to complete order, time of day of orders, recept type (deliver, pick up)\n",
    "\n",
    "For customer level, area of customer, time length of a order (in call, in website), new or existing customer\n",
    "\n",
    "For environmental level, day of the week, temperature, weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf70f9-0e4d-41a7-84bf-bcf2cbb527ff",
   "metadata": {},
   "source": [
    "## Search Algorithm Recall\n",
    "\n",
    "### Question\n",
    "\n",
    "Let's say you work as a data scientist at Amazon. You want to improve the search results for product search but cannot change the underlying logic in the search algorithm. What methods could you use to increase recall?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Recall is\n",
    "\n",
    "$$\n",
    "\\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Reduce threshold to predict more positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bc986-ae74-4593-b6e6-e8501350f148",
   "metadata": {},
   "source": [
    "## Missing housing date\n",
    "\n",
    "### Question\n",
    "\n",
    "We want to build a model to predict housing prices in a city. We've scraped 100,000 listings over the past 3 years but found that around 20% of the listings are missing square footage data. How do we deal with the missing data to construct our model?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Idea of model **without square footage**. I assume the square footage is an important feature to predict housing prices. But we can explore whether we can build a model without a square footage. For example, we remove the rows of data with the missing square footage, and build 2 models; one with square footage and with other features, and the other model without square footage but with the other features. If model accuracy doesn't decrease so much without square footage, we could build a model without square footage. But it's unlikely though.\n",
    "\n",
    "Idea of **deleting** the rows of data with missing square footage. We would remove the rows of data with missing square footage. I call this data 80% data. Build 2 models; one with this 80% data with complete square footage, and the other with reducing another 20% data (I cann this data 60% data) and build the same stats model with complete square footage. If model accuracy from 60% data is slightly less accurate than 80% data model, we may be able to just delete the rows of data with missing square footage.\n",
    "\n",
    "Idea of not delete data and **impute** missing values. We could fill in the missing values with estimations. Simple estimation is the mean or median of the distribution of square footage. Another estimation is **k nearest neighbors algorithm** to approximate a square footage based on grouping different categorical features. We can also get means from different subsets of data by group by data by other features such as locations, number of bedrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ef4d5-2fda-4cd0-8ab7-207ecfd4a90d",
   "metadata": {},
   "source": [
    "## Variable error\n",
    "\n",
    "### Question\n",
    "\n",
    "You have a logistic model that is heavily weighted on one variable. The sample data from the variable is like 50.00, 100.00, 40.00, etc... There was a data quality issue with this variable, and an unknown number of values removed the decimal point. For example, 100.00 turned into 100000. Would the model still be valid? Why or why not? How would you fix the model?\n",
    "\n",
    "### Answer\n",
    "\n",
    "It's not valid, because only unknown subset of values removed decimal points. These affect the estimated parameter of the logistic regression. But if all the data removed the decimal points, the model is valid. The parameter will change but can predict.\n",
    "\n",
    "To fix the model, make a distribution histrogram of the variable and we should be able to see 2 portions of data. The portion of data with large number should be the error data, so divide them by 100 to fix the values. Train model again. We should be able to visually identify where the error has occurred and correct it. \n",
    "\n",
    "However, when the data has a large range like 0 to 1,000,000, the histogram won't show the difference so that it's difficult to catch the error. In such case, we need to use the other variables together and apply clustering such as **expectation maximization** to identify the error data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf93e4-9163-47d6-a9ab-19ebe906b537",
   "metadata": {},
   "source": [
    "## Skewed pricing\n",
    "\n",
    "### Question\n",
    "\n",
    "Buildinga model to predict real estate home prices in a particular city. Analyze the distribution of the home prices and see that the homes values are skewed to the right. Do we need to do anything or take it into consideration? If so, what should we do? If the target distribution is heavily left instead. What do we do now?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Take log to turn the right skewed distribution into the more normal distribution, because typically model assumes the target variable has a normal distribution\n",
    "\n",
    "Multiply by -1 and take log, and train model. In prediction, take exponential of the prediction and multiple by -1.\n",
    "\n",
    "https://www.youtube.com/watch?v=jW0Z4qkR63E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d212d67-b522-44a6-b3a8-303aedf6e3a7",
   "metadata": {},
   "source": [
    "## Bank fraud model\n",
    "\n",
    "### Question\n",
    "\n",
    "You work at a bank that wants to build a model to detect fraud on the platform. The bank wants to implement a text messaging service in addition that will text customers when the model detects a fraudulent transaction in order for the customer to approve or deny the transaction with a text response. How would be build this model?\n",
    "\n",
    "### Answer\n",
    "\n",
    "In this problem, we are going to build a binary classification model on an imbalanced dataset.\n",
    "\n",
    "Things that we need to clarify before building a model is,\n",
    "- How accurate the fraud class data? Is it definitive fraud or suspicious transactions?\n",
    "- Do we care about interpretability of a model to learn how a data will be predicted to be fraud?\n",
    "- What's the cost of misclassification?\n",
    "- Recall or precision, which do we care?\n",
    "- What model works on an imbalanced dataset?\n",
    "\n",
    "In this problem, low **recall** means large false negatives. It means we blindly accept many fraud transactions. It's economically bad. Low **precision** means large false positives. It means we send many texts to customer about fraud, but most of the notifications are wrong. Maybe customers are annoyed by our service, but we are not making direct economical loss. So I assume recall should be higher than precision when training a binary classification model.\n",
    "\n",
    "Computing recall in different subset of transaction amount data would be useful. For example, let's say recall is low with the average 10 dollar fraud data. But if recall is high with the average 1,000 dollar fraud data, it's good result.\n",
    "\n",
    "For model choice, we can try boosting and SVM algorithm because it can learn more from the error of minority class. we can also customize loss function to assign more cost for the larger amount of fraud data to prioritize large fraud prediction.\n",
    "\n",
    "For smaller class of fraud data, we can try **SMOTE** and **ADASYN (Adaptive Synthetic Sampling)** to generate synthetic samples for fraud class data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cb221-9394-4a87-b587-f800b85277ba",
   "metadata": {},
   "source": [
    "## Ride requests model\n",
    "\n",
    "You're tasked with building a model to predict if a driver on Uber will accept a ride request or not.\n",
    "1. What algorithm would you use to build this model?\n",
    "2. What are the tradeoffs between different classifiers?\n",
    "3. What features would you use?\n",
    "\n",
    "1. Binary classification. I think class imbalance depends on locations, time, etc\n",
    "2. Linear model has less accurate but more interpretability, machine learning model has more accurate bu less interpretability\n",
    "\n",
    "3. Features would be\n",
    "  - Expected profit\n",
    "  - Expected total time of driving (Affect driver's availability)\n",
    "  - Expected total distance of driving (Affect driver's gas)\n",
    "  - Rider's review\n",
    "  - Destination\n",
    "\n",
    "https://www.youtube.com/watch?v=Dwbgy7cUxk4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecf856-03cd-4647-a888-69f047f81878",
   "metadata": {},
   "source": [
    "## Multicollinearity in regression\n",
    "\n",
    "### Question\n",
    "\n",
    "How would you tackle multicollinearity in multiple linear regression\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "Ridge to reduce coefficient\n",
    "\n",
    "Lasso to reduce the number of features\n",
    "\n",
    "PCA to reduce correlation\n",
    "\n",
    "Manually reduce the number of features\n",
    "\n",
    "- Decide whether we care about multicollinearity. If we only care the accuracy in training data and test data, we are okay to ignore multicolliearity in model\n",
    "- Standardize each independent variable and check again whether correlation still exists. Example of standardization is subtracting mean from values in a column\n",
    "- Reduce the number of independent variables such as by **Ridge**, **Lasso** or manually removing. Or apply dimension reduction by **PCA**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a75f6-5128-4b4d-a288-f32ef22f9aa9",
   "metadata": {},
   "source": [
    "## Keyword bidding\n",
    "\n",
    "### Question\n",
    "\n",
    "Keyword bidding optimization. A dataset with two columns. One column contains keywords to bid. The other column contains the price to pay for the keyword. How would you build a model to bid on a new unseen keyword? \n",
    "\n",
    "### Answer\n",
    "\n",
    "We will create word embeddings. Word embeddings are vectors where words having the same meaning have a similar vector. \n",
    "\n",
    "Prediction is to compute **cosine similarity** between the embedding and the keyword, and we **recommend** the actual prices of similar words with the high cosine similarity to a new keyword to bid.\n",
    "\n",
    "Use the mean of the embedding as additional feature to a model as well as word to predict price.\n",
    "\n",
    "Training **neural networks** gives us the word embeddings.\n",
    "\n",
    "**GloVe embedding** can be used. It's a co-occurrence matrix of all the keywords in a context.\n",
    "\n",
    "**Word2Vec** can be used. It's predicting the context of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d470b-1bec-40c5-a4d0-22aa908c3386",
   "metadata": {},
   "source": [
    "## Booking regression\n",
    "\n",
    "### Question\n",
    "\n",
    "Build a model to predict booking prices on Airbnb. Between linear regression and random forest regression, which model would perform better and why?\n",
    "\n",
    "### Answer\n",
    "\n",
    "The linear regression adn random forest differences\n",
    "- Random forest regression can approximate the complex nonlinear shapes. But linear regression performs better whent the underlying data is linear and has many continous predictors.\n",
    "- Random forest can use many predictors, but with linear regression we need to reduce the number of predictors to avoid overfit.\n",
    "- Random forest can capture complex interactions between predictors, but we need to create interaction variables with linear regression.\n",
    "- For predictor interpretability, we can compute feature importance with random forest. But the predictor coefficients of linear regression is more interpretable.\n",
    "\n",
    "Possible features to predict booking prices on Airbnb\n",
    "- Location features\n",
    "- Calendar features (Seasonality)\n",
    "- Number of bedrooms and bathrooms\n",
    "- Room type like private room, shared, entire home\n",
    "- External demand like conference, events\n",
    "\n",
    "Linear regression still work. For example, creating one linear regression model per location. And manually create interaction effect features like special event feature with the size of room\n",
    "\n",
    "But maybe random forecast performs better if there are lots of features available and we need to predict around the world. The relationship would be non-linear and too complex to manually create complex features for linear regression.\n",
    "\n",
    "If we only have one zipcode column of one city and other simple rental information, linear regression is a good choice and it gives us the interpretability by the features.\n",
    "\n",
    "```\n",
    "Memo\n",
    "\n",
    "Linear regression\n",
    "- Pros\n",
    "  - We can know how feature values affec model output\n",
    "  - Prediction is faster than random forest\n",
    "  - Perform well if the subset of data is small\n",
    "  - Faster training\n",
    "- Cons\n",
    "  - Less accurate than random forest\n",
    "\n",
    "Random forest\n",
    "- Pros\n",
    "  - More accurate\n",
    "- Cons\n",
    "  - Less interpretable than linear regression, but we can show feature importance\n",
    "  - Slower training but it can be faster in a parellel training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd037a3f-23ff-46fc-8f6e-c13180cc7593",
   "metadata": {},
   "source": [
    "## Credit card fraud model\n",
    "\n",
    "### Question\n",
    "\n",
    "Say you work at a major credit card company and are given a dataset of 600,000 credit card transactions. Use this dataset to build a fraud detection model.\n",
    "\n",
    "### Answer\n",
    "\n",
    "Clarify the definition of a transaction which was labeled as a fraud; User decided or bank decided? \n",
    "\n",
    "Check how frequent the fraud data is in the dataset. Probably, fraud is small, so implement rebalancing methods, such as up-sampling, down-sampling or SMOTE.\n",
    "\n",
    "Perform feature engineering, such as time of data for transaction, \n",
    "\n",
    "https://www.interviewquery.com/learning-paths/modeling-and-machine-learning/model-selection/credit-card-fraud-model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
