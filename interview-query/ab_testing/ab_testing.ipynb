{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0262f0ee",
   "metadata": {},
   "source": [
    "# A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46641beb",
   "metadata": {},
   "source": [
    "## Button AB test\n",
    "\n",
    "### Question\n",
    "\n",
    "A team wants to A/B test multiple different changes through a sign-up funnel.\n",
    "\n",
    "For example, on a page, a button is currently red and at the top of the page. They want to see if changing a button from red to blue and/or from the top of the page to the bottom of the page will increase click-through.\n",
    "\n",
    "How would you set up this test?\n",
    "\n",
    "### Answer\n",
    "\n",
    "We have 2 variables to test button color and button location. We want to have a test that tells us an interaction effect of the 2 variables. There are the following 4 variants.\n",
    "\n",
    "- Red button at the top\n",
    "- Red button at the bottom\n",
    "- Blue button at the top\n",
    "- Blue button at the bottom\n",
    "\n",
    "More variants increase the variance of the results. To set up this test, we should have a **long duration of time of the test** to reduce the variance.\n",
    "\n",
    "It's also possible to set up a chain of the A/B tests. First, we run the color change test for a certain duration of time, and then run the location change test afterward. But this can't observe the interaction effect. For example, if blue is better, and then top is better, we won't know whether red top is better than blue top.\n",
    "\n",
    "We also need to compute the **sample size**. Multiply the **number of page visitors** per day in each variant by the **number of days** to run a test to reach a certain **significance**.\n",
    "\n",
    "Each page visitor needs to be assigned to one of variants to remove bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204396e3",
   "metadata": {},
   "source": [
    "## New UI effect\n",
    "\n",
    "### Question\n",
    "\n",
    "Let’s say we’re testing a new UI with the goal to increase conversion rates. We test it by giving the new UI to a random subset of users.\n",
    "\n",
    "The test variant wins by 5% on the target metric. What would you expect to happen after the new UI is applied to all users? Will the metric actually go up by ~5%, more, or less?\n",
    "\n",
    "Note: Assume there is no novelty effect.\n",
    "\n",
    "### Answer\n",
    "\n",
    "How long and when did the test run? If the test ran on weekends only, we would need to check fi the user behavior differe from at other times of the week.\n",
    "\n",
    "What was the confidence interval, and significance level the test used? It's good if the interval is narrow. It's good if the significance level is 5% or below and the test result satisfies it.\n",
    "\n",
    "Was the sample population a good representative of the whole? Suppose that control group is the existing old users and the treatment group is the new recent users. Are we going to apply the new UI only to new users, or do we plan to apply new UI to both groups?\n",
    "\n",
    "Was there an external factor to push conversion rate?\n",
    "\n",
    "Was there any other experiment working at the same time?\n",
    "\n",
    "What was the **effect size**? If the effect size is in the confidence interval, the test is less relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79d811",
   "metadata": {},
   "source": [
    "## Testing price increase\n",
    "\n",
    "### Question\n",
    "\n",
    "Let’s say that you work at a B2B SAAS company that’s interested in testing the pricing of different levels of subscriptions.\n",
    "\n",
    "Your project manager comes to you and asks you to run a two-week-long A/B test to test an increase in pricing.\n",
    "\n",
    "How would you approach designing this test? How would you determine whether the increase in pricing is a good business decision?\n",
    "\n",
    "### Answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6027a",
   "metadata": {},
   "source": [
    "## Hundreds of hypotheses\n",
    "\n",
    "### Question\n",
    "\n",
    "You are testing hundreds of hypotheses with many t-tests. What considerations should be made?\n",
    "\n",
    "### Answer\n",
    "\n",
    "When we run multiple t-tests, it increases the chance of getting false positive. It's type I error. When false positive probability is $a$, the probability of not getting false positive is $(1 - a)$. When we run $n$ t-tests, the probability of never getting false positive in n tests is $(1 - a)^n$. It's very small, so we are likely to get false positive.\n",
    "\n",
    "We can try **Bonferroni correction** or **F-test**.\n",
    "\n",
    "In Bonferroni correction, when we run $n$ tests with significance level $\\alpha$, we use a significance level $\\frac{\\alpha}{n}$ instead. Even if all the hypotheses are false positive, the significance level is $\\alpha$.\n",
    "\n",
    "Running one F-test is an alternative to multiple t-tests. We don't have to worry about correction. But F-test only tests us whether there is a difference in mean between at least 2 groups, and it doesn't tell us how much differ, which groups, how many groups.\n",
    "\n",
    "The disadvantage of both approaches is to increase false negative. It's type II error. It's problematic if false positive is preferable to false negative. For example, in healthcare, false positive result of detecting disease gives concern and more tests might be required, but false negative means ignoring a potentially fetal disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf14ff",
   "metadata": {},
   "source": [
    "## AB test ties\n",
    "\n",
    "### Question\n",
    "\n",
    "What are the pros and cons of user-tied test vs. user-united test?\n",
    "\n",
    "### Answer\n",
    "\n",
    "For example, the **user-united test** on a search engine splits the traffic at the search level. The search engine typically doesn't require sign-in. So in AB testing, 2 groups are not bucketized based on users. Each instance is just randomly assigned into 2 groups. But **user-tied test** version of AB testing buckets users into 2 groups based on the user level. The user profile is attached to each user in the user-tied test. So we create 2 groups and randomly assign users who we know they are.\n",
    "\n",
    "Benefit of user-tied test is the ability to track metrics over time. For example, engagement over time. The same person won't see the both versions of tests, so no disrupting user experience.\n",
    "\n",
    "Disadvantage of user-tied test is the bias that users that visit the site more may be more likely to convert to a purchase. This is a bias if it's testing 2 different version of funnels for an ecommerce website.\n",
    "\n",
    "Benefit of user-united test is that it doesn't have the above bias (More visit more conversion). \n",
    "\n",
    "Disadvantage of user-unied test is the inability to track metrics over time. Also, the same person could experience 2 different versions of experiment to disrupt user experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
