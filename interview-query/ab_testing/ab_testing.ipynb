{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0262f0ee",
   "metadata": {},
   "source": [
    "# A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46641beb",
   "metadata": {},
   "source": [
    "## Button AB test\n",
    "\n",
    "### Question\n",
    "\n",
    "A team wants to A/B test multiple different changes through a sign-up funnel.\n",
    "\n",
    "For example, on a page, a button is currently red and at the top of the page. They want to see if changing a button from red to blue and/or from the top of the page to the bottom of the page will increase click-through.\n",
    "\n",
    "How would you set up this test?\n",
    "\n",
    "### Answer\n",
    "\n",
    "We have 2 variables to test button color and button location. We want to have a test that tells us an interaction effect of the 2 variables. There are the following 4 variants.\n",
    "\n",
    "- Red button at the top\n",
    "- Red button at the bottom\n",
    "- Blue button at the top\n",
    "- Blue button at the bottom\n",
    "\n",
    "More variants increase the variance of the results. To set up this test, we should have a **long duration of time of the test** to reduce the variance.\n",
    "\n",
    "It's also possible to set up a chain of the A/B tests. First, we run the color change test for a certain duration of time, and then run the location change test afterward. But this can't observe the interaction effect. For example, if blue is better, and then top is better, we won't know whether red top is better than blue top.\n",
    "\n",
    "We also need to compute the **sample size**. Multiply the **number of page visitors** per day in each variant by the **number of days** to run a test to reach a certain **significance**.\n",
    "\n",
    "Each page visitor needs to be assigned to one of variants to remove bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204396e3",
   "metadata": {},
   "source": [
    "## New UI effect\n",
    "\n",
    "### Question\n",
    "\n",
    "Let’s say we’re testing a new UI with the goal to increase conversion rates. We test it by giving the new UI to a random subset of users.\n",
    "\n",
    "The test variant wins by 5% on the target metric. What would you expect to happen after the new UI is applied to all users? Will the metric actually go up by ~5%, more, or less?\n",
    "\n",
    "Note: Assume there is no novelty effect.\n",
    "\n",
    "### Answer\n",
    "\n",
    "How long and when did the test run? If the test ran on weekends only, we would need to check fi the user behavior differe from at other times of the week.\n",
    "\n",
    "What was the confidence interval, and significance level the test used? It's good if the interval is narrow. It's good if the significance level is 5% or below and the test result satisfies it.\n",
    "\n",
    "Was the sample population a good representative of the whole? Suppose that control group is the existing old users and the treatment group is the new recent users. Are we going to apply the new UI only to new users, or do we plan to apply new UI to both groups?\n",
    "\n",
    "Was there an external factor to push conversion rate?\n",
    "\n",
    "Was there any other experiment working at the same time?\n",
    "\n",
    "What was the **effect size**? If the effect size is in the confidence interval, the test is less relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79d811",
   "metadata": {},
   "source": [
    "## Testing price increase\n",
    "\n",
    "### Question\n",
    "\n",
    "Let’s say that you work at a B2B SAAS company that’s interested in testing the pricing of different levels of subscriptions.\n",
    "\n",
    "Your project manager comes to you and asks you to run a two-week-long A/B test to test an increase in pricing.\n",
    "\n",
    "How would you approach designing this test? How would you determine whether the increase in pricing is a good business decision?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Many people argue that using different prices for the same product causes unfairness, but the price of the product of this B2B SAAS company might not be explict to everyone. Maybe the pricing page of the company website says ask for Sales, so this company always tailermade the pricing for each company. There is possibility that customers might talk about the prices with another customer, so when making contract, the contract should say that the price of the product should not be public.\n",
    "\n",
    "If the company webpage has pricing page, the control group will see the existing pricing and the test group will see the new price. It will cause unfairness, but we could risk our reputation and still conduct the A/B test.\n",
    "\n",
    "We need to assume that this product is not monthly subscription. If we are interested in churn rate, two-week-long is not enough to see how customers react.\n",
    "\n",
    "Because it's B2B SAAS product, I don't assume the purchases are not that high-frequency to make a lot of data of subscriptions. Maybe we will have to run A/B test with a small amount of data.\n",
    "\n",
    "We need to decide what is good for business. For example, decreasing the churn rate, increasing the revenue as function of the number of subscriptions and the price per the subscription.\n",
    "\n",
    "Provide the one-week free trial period to customers with randomly several options of prices. And if customers want to keep using the product, they enter the second week as subscription. We try to find the highest price that gives us the highest number of customers to stay. This cares about the conversion rate, but we expect that lower-prices would have higher conversion rate, but we also measure revenue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6027a",
   "metadata": {},
   "source": [
    "## Hundreds of hypotheses\n",
    "\n",
    "### Question\n",
    "\n",
    "You are testing hundreds of hypotheses with many t-tests. What considerations should be made?\n",
    "\n",
    "### Answer\n",
    "\n",
    "When we run multiple t-tests, it increases the chance of getting false positive. It's type I error. When false positive probability is $a$, the probability of not getting false positive is $(1 - a)$. When we run $n$ t-tests, the probability of never getting false positive in n tests is $(1 - a)^n$. It's very small, so we are likely to get false positive.\n",
    "\n",
    "We can try **Bonferroni correction** or **F-test**.\n",
    "\n",
    "In Bonferroni correction, when we run $n$ tests with significance level $\\alpha$, we use a significance level $\\frac{\\alpha}{n}$ instead. Even if all the hypotheses are false positive, the significance level is $\\alpha$.\n",
    "\n",
    "Running one F-test is an alternative to multiple t-tests. We don't have to worry about correction. But F-test only tests us whether there is a difference in mean between at least 2 groups, and it doesn't tell us how much differ, which groups, how many groups.\n",
    "\n",
    "The disadvantage of both approaches is to increase false negative. It's type II error. It's problematic if false positive is preferable to false negative. For example, in healthcare, false positive result of detecting disease gives concern and more tests might be required, but false negative means ignoring a potentially fetal disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf14ff",
   "metadata": {},
   "source": [
    "## AB test ties\n",
    "\n",
    "### Question\n",
    "\n",
    "What are the pros and cons of user-tied test vs. user-united test?\n",
    "\n",
    "### Answer\n",
    "\n",
    "For example, the **user-united test** on a search engine splits the traffic at the search level. The search engine typically doesn't require sign-in. So in AB testing, 2 groups are not bucketized based on users. Each instance is just randomly assigned into 2 groups. But **user-tied test** version of AB testing buckets users into 2 groups based on the user level. The user profile is attached to each user in the user-tied test. So we create 2 groups and randomly assign users who we know they are.\n",
    "\n",
    "Benefit of user-tied test is the ability to track metrics over time. For example, engagement over time. The same person won't see the both versions of tests, so no disrupting user experience.\n",
    "\n",
    "Disadvantage of user-tied test is the bias that users that visit the site more may be more likely to convert to a purchase. This is a bias if it's testing 2 different version of funnels for an ecommerce website.\n",
    "\n",
    "Benefit of user-united test is that it doesn't have the above bias (More visit more conversion). \n",
    "\n",
    "Disadvantage of user-unied test is the inability to track metrics over time. Also, the same person could experience 2 different versions of experiment to disrupt user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52053772",
   "metadata": {},
   "source": [
    "## Random bucketing\n",
    "\n",
    "### Question\n",
    "\n",
    "In an A/B test, how can you check if assignment to the various buckets was truly random?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Find out no huge difference in distributions between the 2 groups for the features that are not related to A/B testing.\n",
    "\n",
    "For example, if we are testing a click rate of a landing page, we can check the traffic origin distribution. If variant A mainly come from search engine, but if variant B mainly come from some ads, probably not randomized.\n",
    "\n",
    "For example, if we are testing a new feature in chat app, we can check the distribution of user attributions. If we have a feature gender containing either man or woman, in random assignment, we expect to have approximately 50% men 50% women distribution, but if variant A contains 80% men, we can doubt randomization. Other user attributions that we can check are user device types, geolocation, etc.\n",
    "\n",
    "We can also check metrics which are not related to the effect that the A/B testing tests. If we are testing a conversion rate in A/B testing, we can compare distribution of time a user spends in a web page, number of purchases between variant A and variant B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860b300",
   "metadata": {},
   "source": [
    "## Sample size bias\n",
    "\n",
    "### Question\n",
    "\n",
    "Let’s say you have to analyze the results of an AB test.\n",
    "\n",
    "One variant of the AB test has a sample size of 50K users and the other has a sample size of 200K users.\n",
    "\n",
    "Given the unbalanced size between the two groups, can you determine if the test will result in bias towards the smaller group?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Check how the data for A/B testing was collected. If collected during different time period, bias could exist. \n",
    "\n",
    "Check the variances of 2 groups. If too different, randomization was not successful.\n",
    "\n",
    "Even if it's unbalanced size, smaller group has a large sample size of 50K.\n",
    "\n",
    "We might not see bias if we downsample the larger group to 50K and run A/B testing.\n",
    "\n",
    "If the different sample size gives us the different variances, and if we run a test assuming the equal variances, we might have bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f4384",
   "metadata": {},
   "source": [
    "## Non-normal AB testing\n",
    "\n",
    "### Question\n",
    "\n",
    "Uber Rider deals with huge amounts of data but at Uber Fleet, the amount of data for experimentation is rather low. Let’s say you run an AB test for Uber Fleet and find out that the distribution is not normal.\n",
    "\n",
    "What kind of analysis would you run and how would you measure which variant won?\n",
    "\n",
    "### Answer\n",
    "\n",
    "We cannot run t-test because the distribution is not normal.\n",
    "\n",
    "We can run **Mann-Whitney U-test** instead because it doesn't assume normal distribution.\n",
    "\n",
    "We can apply resampling permutation test. We shuffle data and sample them, and compute test statistic, and repeat this many times. And compare the observed test statistic with this resampled distribution.\n",
    "\n",
    "Maybe it's not normal because the data is not enough. Maybe it will be normal if we collect more data. If time and budged are allowed, we can collect more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
