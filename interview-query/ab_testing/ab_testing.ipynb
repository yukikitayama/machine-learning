{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0262f0ee",
   "metadata": {},
   "source": [
    "# A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46641beb",
   "metadata": {},
   "source": [
    "## Button AB test\n",
    "\n",
    "### Question\n",
    "\n",
    "A team wants to A/B test multiple different changes through a sign-up funnel.\n",
    "\n",
    "For example, on a page, a button is currently red and at the top of the page. They want to see if changing a button from red to blue and/or from the top of the page to the bottom of the page will increase click-through.\n",
    "\n",
    "How would you set up this test?\n",
    "\n",
    "### Answer\n",
    "\n",
    "We have 2 variables to test button color and button location. We want to have a test that tells us an interaction effect of the 2 variables. There are the following 4 variants.\n",
    "\n",
    "- Red button at the top\n",
    "- Red button at the bottom\n",
    "- Blue button at the top\n",
    "- Blue button at the bottom\n",
    "\n",
    "More variants increase the variance of the results. To set up this test, we should have a **long duration of time of the test** to reduce the variance.\n",
    "\n",
    "It's also possible to set up a chain of the A/B tests. First, we run the color change test for a certain duration of time, and then run the location change test afterward. But this can't observe the interaction effect. For example, if blue is better, and then top is better, we won't know whether red top is better than blue top.\n",
    "\n",
    "We also need to compute the **sample size**. Multiply the **number of page visitors** per day in each variant by the **number of days** to run a test to reach a certain **significance**.\n",
    "\n",
    "Each page visitor needs to be assigned to one of variants to remove bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d898a66",
   "metadata": {},
   "source": [
    "## New UI effect\n",
    "\n",
    "### Question\n",
    "\n",
    "Let’s say we’re testing a new UI with the goal to increase conversion rates. We test it by giving the new UI to a random subset of users.\n",
    "\n",
    "The test variant wins by 5% on the target metric. What would you expect to happen after the new UI is applied to all users? Will the metric actually go up by ~5%, more, or less?\n",
    "\n",
    "Note: Assume there is no novelty effect.\n",
    "\n",
    "### Answer\n",
    "\n",
    "How long and when did the test run? If the test ran on weekends only, we would need to check fi the user behavior differe from at other times of the week.\n",
    "\n",
    "What was the confidence interval, and significance level the test used? It's good if the interval is narrow. It's good if the significance level is 5% or below and the test result satisfies it.\n",
    "\n",
    "Was the sample population a good representative of the whole? Suppose that control group is the existing old users and the treatment group is the new recent users. Are we going to apply the new UI only to new users, or do we plan to apply new UI to both groups?\n",
    "\n",
    "Was there an external factor to push conversion rate?\n",
    "\n",
    "Was there any other experiment working at the same time?\n",
    "\n",
    "What was the **effect size**? If the effect size is in the confidence interval, the test is less relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
