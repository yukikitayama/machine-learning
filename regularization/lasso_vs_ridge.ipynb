{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e551f8",
   "metadata": {},
   "source": [
    "# Lasso vs Ridge\n",
    "\n",
    "What's the difference between Lasso and Ridge Regression?\n",
    "\n",
    "Both add a penalty term to the loss functions of regression to prevent over-fitting.\n",
    "\n",
    "$p$ is the number of parameters. $L$ is the loss function.\n",
    "\n",
    "Lasso adds **1-norm of parameters** to the loss function\n",
    "\n",
    "$$\n",
    "L_{lasso}(\\beta | X, y, \\alpha)\n",
    "$$\n",
    "$$\n",
    "= L(\\beta | X, y) + \\alpha || \\beta ||_1^1\n",
    "$$\n",
    "$$\n",
    "= L(\\beta | X, y) + \\alpha \\sum_{i = 1}^{p} | \\beta_i |\n",
    "$$\n",
    "\n",
    "Ridge adds **2-norm of parameters** to the loss function\n",
    "\n",
    "$$\n",
    "L_{ridge} (\\beta | X, y, \\alpha)\n",
    "$$\n",
    "$$\n",
    "= L(\\beta | X, y) + \\frac{\\alpha || \\beta ||_2^2}{2}\n",
    "$$\n",
    "$$\n",
    "= L(\\beta | X, y) + \\frac{\\alpha}{2} \\sqrt{ \\sum_{i = 1}^{p} \\beta_i^2}\n",
    "$$\n",
    "\n",
    "Lasso makes some parameters be 0 so that it can work as variable selection. It's because of the **diamond shape** of Lasso penalty term. When one of parameters is 0, the possible parameters touches the edge of the diamond. \n",
    "\n",
    "![lasso_diamond](./lasso_diamond.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
