{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Learning_Reinforcement_Learning_OpenAI_Gym.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9HQWDhO3P4dn","colab_type":"text"},"source":["# Learning Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"ImHmchN_I2d1","colab_type":"text"},"source":["I'm learning reinforcement learning from the following. I think I will say reinforcement learning is a training to get the best action in each state in the state space. But I'm not clear about when a taxi run into a wall and evaluation. <br> \n","https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"]},{"cell_type":"markdown","metadata":{"id":"zVLLklarQrtr","colab_type":"text"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"9KuuxjS_IM9R","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random\n","\n","# reinforcement learning examples in it\n","import gym\n","\n","# visualize training\n","from IPython.display import clear_output\n","\n","# use sleep to get a snapshot and to visualize training process\n","from time import sleep"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5idfLqiwWN-0","colab_type":"text"},"source":["## Self-driving cab"]},{"cell_type":"code","metadata":{"id":"Ox-NYrLaIVIo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"f726718c-d2e8-4fec-c4f1-24838f910aa3","executionInfo":{"status":"ok","timestamp":1570303473799,"user_tz":240,"elapsed":439,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["# taxi example of reinforcement learning\n","env = gym.make(\"Taxi-v2\").env\n","\n","# fix the initial location of a taxi\n","env.s = env.encode(3, 4, 2, 0)\n","# vertical and horizontal states start from 0, so 0, 1, 2, 3, 4 for each\n","\n","# show the taxi and a park\n","env.render()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","| : : : : |\n","| : : : : |\n","| | : | :\u001b[43m \u001b[0m|\n","|\u001b[34;1mY\u001b[0m| : |B: |\n","+---------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NwKU935SIjvk","colab_type":"text"},"source":["Box is a taxi. The initial location of a taxi randomly will be assigned. Yellow means the taxi is empty. Once a passanger takes it, the box becomes black. Alphabets are possible locations for pickup or destination. The blue shows a passener is there, and the pink is the place he wants to go. After Q-learning, we will see what the best action is in this location. Since a passenger is at Y, a taxi should go north or west first step. "]},{"cell_type":"code","metadata":{"id":"WcyPoY7zIqJp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"d9d123cd-e69e-436e-ac83-bdb001eb22ee","executionInfo":{"status":"ok","timestamp":1570303476608,"user_tz":240,"elapsed":364,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["# show the number of action states\n","print(env.action_space)\n","\n","# show the number of observation state, which is 5 * 5 * 4 * (1 + 4) \n","# (Vetical * horizontal * pickup location * passenger-in-the-taxi state and destination)\n","print(env.observation_space)\n","\n","# internal logic\n","print(env.s)\n","print(env.P[env.s])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Discrete(6)\n","Discrete(500)\n","388\n","{0: [(1.0, 488, -1, False)], 1: [(1.0, 288, -1, False)], 2: [(1.0, 388, -1, False)], 3: [(1.0, 368, -1, False)], 4: [(1.0, 388, -10, False)], 5: [(1.0, 388, -10, False)]}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BJe1V7EWNTQO","colab_type":"text"},"source":["env.P is {action: [(probability, next state, reward, done)]}"]},{"cell_type":"markdown","metadata":{"id":"k5nZoY_OW3li","colab_type":"text"},"source":["## Reinforcement learning - Q-learning algorithm"]},{"cell_type":"code","metadata":{"id":"CXVQNVAAcTVl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"1baa8c2c-5253-47f6-f6d5-3871e530835e","executionInfo":{"status":"ok","timestamp":1570298775629,"user_tz":240,"elapsed":361,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["# initialize state action space matrix to store learned values later\n","\n","# possibilities for all the state spaces and action spaces\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","print(q_table.shape)\n","\n","# action states = {south, north, east, west, pickup, dropoff}"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(500, 6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yvEvCuVkdG44","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"709b123c-a245-40cb-929b-1056e3705f12","executionInfo":{"status":"ok","timestamp":1570298823845,"user_tz":240,"elapsed":10,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["# show training time\n","%%time\n","\n","# hyperparameters for q learning\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","# use grid search to know these values if you want to\n","\n","for i in range(1, 100001):\n","  \n","  # initialize by reset\n","  state = env.reset()\n","  epochs = 0\n","  penalties = 0\n","  reward = 0\n","  done = False\n","  \n","  # done is successful passenger dropoff\n","  while not done:\n","    \n","    # using epsilon logic deliberately avoid taking best route many time\n","    # it can reduce overfitting.\n","    if random.uniform(0, 1) < epsilon:\n","      # explore action space\n","      action = env.action_space.sample()\n","      # deliberately sample action states from 0,1,2,3,4,5 to explore new possibilities\n","      \n","    else:\n","      # exploit learned values\n","      action = np.argmax(q_table[state])\n","      # draw best action\n","      \n","    # draw following info from the action we decided\n","    next_state, reward, done, info = env.step(action)\n","    \n","    # draw q value in a certain state and with a certain action\n","    old_value = q_table[state, action]\n","    \n","    # we already took action, so we have next state\n","    # np.max draw the maximum q value from each action in a certain state\n","    next_max = np.max(q_table[next_state])\n","    \n","    # The most important q value update algorithm for Q-learning\n","    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","    # It's a combination of reward of current action in current state \n","    # and discounted maximum reward from next state from current action\n","    \n","    # update Q-table of a certain state of a certain action\n","    q_table[state, action] = new_value \n","    \n","    # if making a big mistake, accumulate penalty\n","    if reward == -10:\n","      penalties += 1\n","     \n","    # for iteration\n","    state = next_state\n","    epochs += 1\n","    \n","  if i % 100 == 0:\n","    clear_output(wait = True)\n","    print(f\"Episode: {i}\")\n","    \n","print(\"Training finished.\\n\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Episode: 100000\n","Training finished.\n","\n","CPU times: user 44.3 s, sys: 2.81 s, total: 47.2 s\n","Wall time: 45.9 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PCKp9e0SY64s","colab_type":"text"},"source":["## Check our initial intuition"]},{"cell_type":"code","metadata":{"id":"2yItWZ_KGFYe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"06b08fe8-b456-496d-f081-80d8002984f4","executionInfo":{"status":"ok","timestamp":1570303485332,"user_tz":240,"elapsed":341,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["q_table[388]\n","# largest value action state is north"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-2.4737387 , -2.4510224 , -2.4594717 , -2.4510224 , -9.6392277 ,\n","       -9.64972901])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"PV8PPCggY-Md","colab_type":"text"},"source":["The highest values in Q table is the second and forth one, which are north and west, so we got the right answers."]},{"cell_type":"markdown","metadata":{"id":"xHXzrGcph0tr","colab_type":"text"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"bgQhCzbDI8AS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"d0df6bc5-4d7a-49db-98f7-d5d9cee049aa","executionInfo":{"status":"ok","timestamp":1570299791536,"user_tz":240,"elapsed":391,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["# evaluation\n","total_epochs = 0\n","total_penalties = 0\n","episodes = 100\n","\n","for _ in range(episodes):\n","  state = env.reset()\n","  epochs = 0\n","  penalties = 0\n","  reward = 0\n","  done = False\n","  \n","  while not done:\n","    action = np.argmax(q_table[state])\n","    state, reward, done, info = env.step(action)\n","    \n","    if reward == -10:\n","      penalties += 1\n","      \n","    epochs += 1\n","    \n","  total_penalties += penalties\n","  total_epochs += epochs\n","  \n","print(episodes)\n","print(total_epochs / episodes)\n","print(total_penalties / episodes)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["100\n","12.07\n","0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wyapGnFFXwwL","colab_type":"text"},"source":["## Appendix"]},{"cell_type":"code","metadata":{"id":"SX_t2QpqPlAG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"0e411cfd-06fc-4c50-a970-5893d0bac084","executionInfo":{"status":"ok","timestamp":1570298491749,"user_tz":240,"elapsed":384,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["# non-reinforcement learning example\n","env.s = 328\n","epochs = 0\n","penalties = 0\n","reward = 0\n","frames = []\n","done = False\n","\n","while not done:\n","  action = env.action_space.sample()\n","  state, reward, done, info = env.step(action)\n","  \n","  if reward == -10:\n","    penalties += 1\n","    \n","  frames.append({'frame': env.render(mode = 'ansi'),\n","                 'state': state,\n","                 'action': action,\n","                 'reward': reward})\n","  \n","  epochs += 1\n","\n","print(epochs)\n","print(penalties)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["256\n","58\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i97Vp31NQndo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"f507e215-3e9d-4817-91ac-88e937054627","executionInfo":{"status":"ok","timestamp":1570284920630,"user_tz":240,"elapsed":36,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["# visualize moving process\n","def print_frames(frames):\n","  for i, frame in enumerate(frames):\n","    clear_output(wait = True)\n","    print(frame['frame'].getvalue())\n","    print(f\"Timestep: {i + 1}\")\n","    print(f\"State: {frame['state']}\")\n","    print(f\"Action: {frame['action']}\")\n","    print(f\"Reward: {frame['reward']}\")\n","    # to show a snapshop, delay the next image update by sleep function from time module\n","    sleep(.1)\n","  \n","# when a box becomes black, it means a taxi picks up a passenger\n","print_frames(frames)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n","| : : : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |B: |\n","+---------+\n","  (Dropoff)\n","\n","Timestep: 296\n","State: 0\n","Action: 5\n","Reward: 20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F0d_auRMfLiR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"fce0cf53-0de7-460b-ced9-c8c088b8a4ea","executionInfo":{"status":"ok","timestamp":1570305523810,"user_tz":240,"elapsed":406,"user":{"displayName":"Yuki Kitayama","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64","userId":"10996233055866244291"}}},"source":["print(env.action_space)\n","action = env.action_space.sample()\n","print(action) \n","\n","# next_state, reward, done, info = env.step(action)\n","print(env.step(action))\n","\n","print(env.reset())"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Discrete(6)\n","3\n","(468, -1, False, {'prob': 1.0})\n","326\n"],"name":"stdout"}]}]}