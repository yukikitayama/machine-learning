{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a25c9c9",
   "metadata": {},
   "source": [
    "# Neural Network Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2895f0",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Adding regularization to the cost function of neural network is,\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(W, b) = \\frac{1}{m} \\sum_{i = 1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l = 1}^{L} \\| W^{[l]} \\|^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\| W^{[l]} \\|_{F}^2 = \\sum_{i = 1}^{n^{[l]}} \\sum_{j = 1}^{n^{[l - 1]}} (w_{i, j}^{[l]})^2\n",
    "$$\n",
    "\n",
    "Because we wanna minimize $\\mathcal{J}$, if we set a large $\\lambda$, $W$ will be close to 0, because otherwise, $\\mathcal{J}$ won't be small.\n",
    "\n",
    "We have double sums because $W$ is a **matrix** with weights, and its dimension is,\n",
    "\n",
    "$$\n",
    "W: (n^{[l]} \\times n^{[l - 1]})\n",
    "$$\n",
    "\n",
    "So in one weight matrix, we square each element of weights and sum up to all the rows and columns directions to make it a single **scalar**. This $\\| W^{[l]} \\|_{F}^2$ is called **Frobenius norm**.\n",
    "\n",
    "**Gradient descent** with regularization will be the following. First, we get derivative of cost function with respect to weight. $\\alpha$ is a learning rate. $\\lambda$ is a **regularization parameter** to control how much effective the regularization is. $m$ is the number of examples. Let $\\text{backprops}$ be a sequence of partial derivatives obtained in **backward propagation**.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{J}}{\\partial W} = \\text{backprops} + \\frac{\\lambda}{m} W\n",
    "$$\n",
    "\n",
    "So the gradient descent will be,\n",
    "\n",
    "$$\n",
    "W = W - \\alpha \\frac{\\partial \\mathcal{J}}{\\partial W}\n",
    "$$\n",
    "$$\n",
    "= W - \\alpha \\left[ \\text{backprops} + \\frac{\\lambda}{m} W \\right]\n",
    "$$\n",
    "$$\n",
    "= W - \\alpha \\frac{\\lambda}{m} W - \\alpha \\left[ \\text{backprops} \\right]\n",
    "$$\n",
    "$$\n",
    "= (1 - \\frac{\\alpha \\lambda}{m}) W - \\alpha \\left[ \\text{backprops} \\right]\n",
    "$$\n",
    "\n",
    "Typically $\\frac{\\alpha \\lambda}{m} \\ll 1$, so with the regularization parameter, it has an effect of making $W$ smaller, because subtracting some number from 1. It is the same effect as we see in the regularization in linear regression to make paramters small. Because of regularization, weights get smaller, so some people say it's **weight decay**. \n",
    "\n",
    "**Dropout** is another to introduce regularization by making a neural network small by randomly making some weights 0. \n",
    "\n",
    "In image recognition tasks, **data augmentation** can be used as regularization by adding noise, changing angles, zoom in/out and flipping, then we can add more challenging data or diverse data to training data.\n",
    "\n",
    "https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aef2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95db12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8eb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
