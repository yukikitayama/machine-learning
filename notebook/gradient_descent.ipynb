{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6958faf",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcc3ed",
   "metadata": {},
   "source": [
    "- Start with some $\\theta$\n",
    "- Keep changing $\\theta$ to reduce cost function $J(\\theta)$\n",
    "- Repeat until it ends up at a minimum\n",
    "\n",
    "Gradient descent algorithm\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "$j$ is index for $p$ features. Repeat it until convergence. For each parameter, take derivative of cost function with respect to each parameter. Simultaneously update all the j $\\theta$'s\n",
    "\n",
    "Correct (Simultaneous update)\n",
    "\n",
    "temp0 = $\\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta)$\n",
    "\n",
    "temp1 = $\\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta)$\n",
    "\n",
    "$\\theta_0 = $ temp0\n",
    "\n",
    "$\\theta_1 = $ temp1\n",
    "\n",
    "Incorrect (Not simultaneous update)\n",
    "\n",
    "temp0 = $\\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta)$\n",
    "\n",
    "$\\theta_0 = $ temp0\n",
    "\n",
    "temp1 = $\\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta)$\n",
    "\n",
    "$\\theta_1 = $ temp1\n",
    "\n",
    "Gradient descent can converge to a local minimum even with the learning rate $\\alpha$ **fixed**. As it approaches a local minimum, gradient gets smaller, so gradient descent will take smaller steps. So no need to decrease $\\alpha$ over time.\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "Each step of gradient descent uses **all** the training exmaples. It's **confusing** because batch sounds like a part of the data, but it actually uses all the data.\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "If we make sure multiple features are on a **similar scales**, meaning having similar ranges of values, then gradient descent can **converge more quickly**.\n",
    "\n",
    "Contour of loss function (cost function) becomes a skewed elliptical shape. Gradient is likely to oscillate and take a long time to reach a global optimum, because gradient is **perpendicular** to contour.\n",
    "\n",
    "By scaling, contour becomes circle. Gradient descent is less likely to have oscillation.\n",
    "\n",
    "For example, get every feature into approximately a $-1 \\le x \\le 1$ range.\n",
    "\n",
    "**Mean normalization** is to replace $x$ with $x - \\mu$ to make features have approximately 0 mean (Do not apply to intercept $x_0 = 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428d0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623fbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b84be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b82e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
