{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b087c50",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b81395",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "If network has $s_j$ units in layer $j$ and $s_{j + 1}$ units inlayer $j + 1$, then $\\Theta^{(j)}$ will be of dimension $s_{j + 1} \\times (s_{j} + 1)$. $+ 1$ because $s_{j + 1}$ has additional **bias unit**. $\\Theta^{(j)}$ is a matrix of **weights** controlling function mapping from layer $j$ to layer $j + 1$.\n",
    "\n",
    "When a neural network has **no hidden layers** and has only **one unit in output layer**,\n",
    "\n",
    "- If output layer is **linear activation**, it's **linear regression** because $y = I (\\Theta x) = \\Theta x$.\n",
    "- If output layer is **sigmoid activation**, it's **logistic regression** because $y = \\sigma(\\Theta x)$ where $\\sigma = \\frac{1}{1 + e^{(-\\Theta x)}}$.\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "In **multi-class classification** where $n$ is the number of data, $L$ is the number of layers in neural network including input and output layers, $s_{l}$ is the number of units (not including bias unit) in layer $l$, $K$ is the number of classes, $\\Theta$ is the weight matrices, $h_{\\Theta}(x)$ is the output of neural network and $\\in \\mathbb{R}^K$, $(h_{\\Theta}(x))_i$ is $i^{th}$ output, and $J(\\Theta)$ is the cost.\n",
    "\n",
    "$$\n",
    "J(\\Theta) = - \\frac{1}{n} \\left[ \\sum_{i = 1}^{n} \\sum_{k = 1}^{K} y_{k}^{(i)} \\log (h_{\\Theta}(x^{(i)}))_{k} + (1 - y_{k}^{(i)}) \\log (1 - (h_{\\Theta}(x^{(i)}))_{k}) \\right] + \\frac{\\lambda}{2n} \\sum_{l = 1}^{L} \\sum_{i = 1}^{s_{l}} \\sum_{j = 1}^{s_{l + 1}} (\\Theta_{ji}^{(l)})^2\n",
    "$$\n",
    "\n",
    "This math takes the form of,\n",
    "\n",
    "$$\n",
    "\\text{Regularized cost} = \\text{Cost} + \\lambda \\times \\text{Regularization}\n",
    "$$\n",
    "\n",
    "The first $\\sum_{i = 1}^{n} \\sum_{k = 1}^{K} y_{k}^{(i)}$ part says that we get the **log-likelihood** by each class and sum up all the $n$ items and divide it by $n$ to get the average cost.\n",
    "\n",
    "The second $\\sum_{l = 1}^{L} \\sum_{i = 1}^{s_{l}} \\sum_{j = 1}^{s_{l + 1}}$ says that we get all the weight parameters in the neural network to regularize them.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "**Backpropagation** is neural network terminology for minimizing the cost function. The goal is to compute,\n",
    "\n",
    "$$\n",
    "\\underset{\\Theta}{\\min} J(\\Theta)\n",
    "$$\n",
    "\n",
    "It means that we want to minimize the cost function $J$ using an optimal set of parameters $\\Theta$.\n",
    "\n",
    "\n",
    "## Resource\n",
    "\n",
    "- [Machine Learning by Stanford University | Coursera](https://www.coursera.org/learn/machine-learning)\n",
    "- [Deep Learning Specialization | Coursera](https://www.coursera.org/specializations/deep-learning)\n",
    "\n",
    "## Note\n",
    "\n",
    "$X = (p \\times n)$, $Y = (1 \\times n)$\n",
    "\n",
    "Logistic regression $\\hat{y} = \\sigma(w^T x + b)$\n",
    "\n",
    "Loss function is for single data error, $l(\\hat{y^{(i)}}, y^{(i)})$\n",
    "\n",
    "Cost function is for sum of loss functions for the entire dataset, $J(w, b)$\n",
    "\n",
    "https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0ULGt/derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57ea22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7119d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629dec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1766738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ee672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
