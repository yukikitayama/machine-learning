{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b087c50",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8777e",
   "metadata": {},
   "source": [
    "## Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19834dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b81395",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "If network has $s_j$ units in layer $j$ and $s_{j + 1}$ units inlayer $j + 1$, then $\\Theta^{(j)}$ will be of dimension $s_{j + 1} \\times (s_{j} + 1)$. $+ 1$ because $s_{j + 1}$ has additional **bias unit**. $\\Theta^{(j)}$ is a matrix of **weights** controlling function mapping from layer $j$ to layer $j + 1$.\n",
    "\n",
    "When a neural network has **no hidden layers** and has only **one unit in output layer**,\n",
    "\n",
    "- If output layer is **linear activation**, it's **linear regression** because $y = I (\\Theta x) = \\Theta x$.\n",
    "- If output layer is **sigmoid activation**, it's **logistic regression** because $y = \\sigma(\\Theta x)$ where $\\sigma = \\frac{1}{1 + e^{(-\\Theta x)}}$.\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "In **multi-class classification** where $n$ is the number of data, $L$ is the number of layers in neural network including input and output layers, $s_{l}$ is the number of units (not including bias unit) in layer $l$, $K$ is the number of classes, $\\Theta$ is the weight matrices, $h_{\\Theta}(x)$ is the output of neural network and $\\in \\mathbb{R}^K$, $(h_{\\Theta}(x))_i$ is $i^{th}$ output, and $J(\\Theta)$ is the cost.\n",
    "\n",
    "$$\n",
    "J(\\Theta) = - \\frac{1}{n} \\left[ \\sum_{i = 1}^{n} \\sum_{k = 1}^{K} y_{k}^{(i)} \\log (h_{\\Theta}(x^{(i)}))_{k} + (1 - y_{k}^{(i)}) \\log (1 - (h_{\\Theta}(x^{(i)}))_{k}) \\right] + \\frac{\\lambda}{2n} \\sum_{l = 1}^{L} \\sum_{i = 1}^{s_{l}} \\sum_{j = 1}^{s_{l + 1}} (\\Theta_{ji}^{(l)})^2\n",
    "$$\n",
    "\n",
    "This math takes the form of,\n",
    "\n",
    "$$\n",
    "\\text{Regularized cost} = \\text{Cost} + \\lambda \\times \\text{Regularization}\n",
    "$$\n",
    "\n",
    "The first $\\sum_{i = 1}^{n} \\sum_{k = 1}^{K} y_{k}^{(i)}$ part says that we get the **log-likelihood** by each class and sum up all the $n$ items and divide it by $n$ to get the average cost.\n",
    "\n",
    "The second $\\sum_{l = 1}^{L} \\sum_{i = 1}^{s_{l}} \\sum_{j = 1}^{s_{l + 1}}$ says that we get all the weight parameters in the neural network to regularize them.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "**Backpropagation** is neural network terminology for minimizing the cost function. The goal is to compute,\n",
    "\n",
    "$$\n",
    "\\underset{\\Theta}{\\min} J(\\Theta)\n",
    "$$\n",
    "\n",
    "It means that we want to minimize the cost function $J$ using an optimal set of parameters $\\Theta$.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "### Logistic Regression Gradient Descent\n",
    "\n",
    "$$\n",
    "z = w^T x + b\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = a = \\sigma(z)\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}(a, y) = -(y \\log(a) + (1 - y) \\log(1 - a))\n",
    "$$\n",
    "\n",
    "When $p = 2$,\n",
    "\n",
    "Computation graph is,\n",
    "\n",
    "$x_1, w_1, x_2, w_2, b \\rightarrow z = w_1 x_1 + w_2 x_2 + b \\rightarrow \\hat{y} = a = \\sigma(z) \\rightarrow \\mathcal{L}(a, y)$ \n",
    "\n",
    "By changing $w_1, w_2, b$, we want to reduce $\\mathcal{L}(a, y)$\n",
    "\n",
    "The loss function is,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(a, y) = -(y \\log(a) + (1 - y) \\log(1 - a))\n",
    "$$\n",
    "\n",
    "Derivative of loss function with respect to $a$ is, by derivative of log and chain rule,\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{da} = -y \\frac{1}{a} - (1 - y) \\frac{1}{1 - a} (-1)\n",
    "$$\n",
    "$$\n",
    "= \\frac{-y}{a} + \\frac{1 - y}{1 - a}\n",
    "$$\n",
    "$$\n",
    "= \\frac{-y(1 - a)}{a(1 - a)} + \\frac{(1 - y)a}{(1 - a)a}\n",
    "$$\n",
    "$$\n",
    "= \\frac{-y + ay + a - ay}{a(1 - a)}\n",
    "$$\n",
    "\n",
    "So we have,\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{da} = \\frac{a - y}{a(1 - a)}\n",
    "$$\n",
    "\n",
    "Next, derivative of $a$ with respect to $z$, because $a = \\sigma(z)$,\n",
    "\n",
    "$$\n",
    "\\frac{da}{dz} = \\frac{d}{dz} \\sigma(z)\n",
    "$$\n",
    "\n",
    "Because derivative of sigmoid function is $\\frac{d}{dz} \\sigma(z) = \\sigma(z)(1 - \\sigma(z)$ and $a = \\sigma(z)$,\n",
    "\n",
    "$$\n",
    "\\frac{da}{dz} = a (1 - a)\n",
    "$$\n",
    "\n",
    "Finally, derivative of loss function with respect to $z$ is, by chain rule,\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{dz} = \\frac{d \\mathcal{L}}{da} \\frac{da}{dz}\n",
    "$$\n",
    "$$\n",
    "= \\frac{a - y}{a(1 - a)} a (1 - a)\n",
    "$$\n",
    "\n",
    "So we have,\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{dz} = a - y \n",
    "$$\n",
    "\n",
    "Now, we get derivative with respect to parameters, $\\frac{d \\mathcal{L}}{d w_1}$, $\\frac{d \\mathcal{L}}{d w_2}$, and $\\frac{d \\mathcal{L}}{db}$. By chain rule,\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{d w_1} = \\frac{d \\mathcal{L}}{da} \\frac{da}{dz} \\frac{dz}{dw_1}\n",
    "$$\n",
    "\n",
    "Because $z = w_1 x_1 + w_2 x_2 + b$, derivative of $z$ with respect to $w_1$ is,\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_1} = x_1\n",
    "$$\n",
    "\n",
    "Likewise,\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_2} = x_2\n",
    "$$\n",
    "$$\n",
    "\\frac{dz}{db} = 1\n",
    "$$\n",
    "\n",
    "So finally derivative of loss function is each parameter is,\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{dw_1} = \\frac{a - y}{a(1 - a)} a (1 - a) x_1 = (a - y) x_1\n",
    "$$\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{dw_2} = \\frac{a - y}{a(1 - a)} a (1 - a) x_2 = (a - y) x_2\n",
    "$$\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{db} = \\frac{a - y}{a(1 - a)} a (1 - a) 1 = (a - y)\n",
    "$$\n",
    "\n",
    "Because we got the gradient with respect to parameters, finally we can do gradient descent by\n",
    "\n",
    "$$\n",
    "w_1 = w_1 - \\alpha \\frac{d \\mathcal{L}}{d w_1} = w_1 - \\alpha (a - y) x_1\n",
    "$$\n",
    "$$\n",
    "w_2 = w_2 - \\alpha \\frac{d \\mathcal{L}}{d w_2} = w_2 - \\alpha (a - y) x_2\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha \\frac{d \\mathcal{L}}{d b} = b - \\alpha (a - y)\n",
    "$$\n",
    "\n",
    "### Pseudocode for Gradient Descent in Neural Network\n",
    "\n",
    "When neural network architecture uses logistic regression, $p = 2$, $n$ is the number of data, and use the simplified expression of $dw_1$ for the derivative $\\frac{d \\mathcal{L}}{dw_1}$,\n",
    "\n",
    "```\n",
    "# Initialize variables to accumulate sums to compute average\n",
    "J = 0, dw_1 = 0, dw_2 = 0, db = 0\n",
    "\n",
    "# Iterate each example\n",
    "for i = 1 to n\n",
    "  \n",
    "  # Forward propagation to compute loss\n",
    "  z_i = w x_i + b\n",
    "  a_i = sigma(z_i)\n",
    "  J += -(y_i log(a_i) + (1 - y_i)log(1 - a_i))\n",
    "  \n",
    "  # Backpropagation to compute derivative\n",
    "  dz_i = a_i - y_i\n",
    "  dw_1 += x_1_i dz_i\n",
    "  dw_2 += x_2_i dz_i\n",
    "  db += dz_i\n",
    "  \n",
    "# Compute average\n",
    "J /= n, dw_1 /= n, dw_2 /= n, db /= n \n",
    "\n",
    "# Gradient descent\n",
    "w_1 = w_1 - alpha dw_1\n",
    "w_2 = w_2 - alpha dw_2\n",
    "b = b - alpha db\n",
    "```\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "**Whenever possible, avoid explicit for-loops** in coding neural network\n",
    "\n",
    "## Resource\n",
    "\n",
    "- [Machine Learning by Stanford University | Coursera](https://www.coursera.org/learn/machine-learning)\n",
    "- [Deep Learning Specialization | Coursera](https://www.coursera.org/specializations/deep-learning)\n",
    "\n",
    "## Note\n",
    "\n",
    "$X = (p \\times n)$, $Y = (1 \\times n)$\n",
    "\n",
    "Logistic regression $\\hat{y} = \\sigma(w^T x + b)$\n",
    "\n",
    "Loss function is for single data error, $l(\\hat{y^{(i)}}, y^{(i)})$\n",
    "\n",
    "Cost function is for sum of loss functions for the entire dataset, $J(w, b)$\n",
    "\n",
    "https://www.coursera.org/learn/neural-networks-deep-learning/programming/XaIWT/logistic-regression-with-a-neural-network-mindset\n",
    "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcfb4f5",
   "metadata": {},
   "source": [
    "## Coding Neural Network Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7119d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.9525741268224334\n",
      "0.04742587317756678\n",
      "(array([[0.],\n",
      "       [0.]]), 0)\n",
      "dw = [[0.99993216]\n",
      " [1.99980262]]\n",
      "db = 0.49993523062470574\n",
      "cost = 6.000064773192205\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def initialize_parameters_with_zeros(dim):\n",
    "    w = np.zeros(shape=(dim, 1))\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    n is the number of data. p is the number of features\n",
    "\n",
    "    Argument:\n",
    "    w: (p x 1) weights\n",
    "    b: bias scalar\n",
    "    X: (p x n) input data\n",
    "    Y: (1 x n) output data\n",
    "    \n",
    "    Return:\n",
    "    cost: a scalar, negative log-likelihood\n",
    "    dw: gradient of the loss with respect to w, (p x 1), same shape as w\n",
    "    db: gradient of the loss with respect to b, (1 x 1), same shape as b\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # Forward propagation\n",
    "    # Compute activation function\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    # Compute cost function\n",
    "    cost = (- 1 / n) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    # Backpropagation\n",
    "    dw = (1 / n) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / n) * np.sum(A - Y)\n",
    "    grads = {\n",
    "        'dw': dw,\n",
    "        'db': db\n",
    "    }\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "    \n",
    "print(sigmoid(0))\n",
    "print(sigmoid(3))\n",
    "print(sigmoid(-3))\n",
    "print(initialize_parameters_with_zeros(2))\n",
    "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629dec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1766738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ee672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-machine-learning",
   "language": "python",
   "name": "env-machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
